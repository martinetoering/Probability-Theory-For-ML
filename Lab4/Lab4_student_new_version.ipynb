{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 4\n",
    "\n",
    "\n",
    "* It is expected that you work individually and the usual plagarism rules apply.\n",
    "* Submissions are to be made on canvas. Make sure that you add your student ID in the submission comments.\n",
    "* The main notebook file you submit should read \"Lab[number]_[last name].ipynb\", for example \"Lab2_Bongers.ipynb\". \n",
    "* Please make sure your code will run without problems\n",
    "\n",
    "_You need to fill in everywhere that there is a_ '__TODO__'\n",
    "\n",
    "Feel free ask any questions during the computer lab sessions, or email the TA.\n",
    "\n",
    "**The due date for the labs is next Wednesday at 14:59**\n",
    "\n",
    "## Part 1\n",
    "\n",
    "In this part of the lab you are going to fit the parameters of a Gaussian in two ways\n",
    "- Method 1: Gradient descent on the averaged negative log likelihood\n",
    "- Method 2: Moment matching\n",
    "\n",
    "**You do not need to fill in a lot of code, but make sure you read everything and understand what is going on**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "import numpy as np\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "from scipy.optimize import minimize\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we generate the data from a Gaussian with random mean and variance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 10\n",
    "mean_true = np.random.randn()\n",
    "var_true = 1 + np.random.rand()\n",
    "data = mean_true + np.sqrt(var_true)*np.random.randn(N)\n",
    "\n",
    "# Plotting\n",
    "datamin = np.amin(data)\n",
    "datamax = np.amax(data)\n",
    "absmax = np.maximum(np.abs(datamin), np.abs(datamax)) + 1\n",
    "plt.figure(0)\n",
    "plt.scatter(data, np.zeros(data.shape))\n",
    "plt.xlim(-absmax, absmax)\n",
    "plt.title('Scatter plot of training data $\\mathcal{D}$', fontsize=16)\n",
    "plt.xlabel('x', fontsize=16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO** Write a function which returns the averaged negative log-likelihood (denoted $\\mathcal{L}$) of the parameters $\\mu$ and $\\sigma$ of a 1D Gaussian given $N$ data points i.e. $$\\texttt{NLL}(\\{x_i\\}, \\texttt{mu}, \\texttt{sigma}) = \\mathcal{L}(\\mu,\\sigma) = -\\frac{1}{N} \\sum_{i=1}^N \\log \\left ( \\frac{1}{\\sigma \\sqrt{2\\pi}} \\exp \\left \\{ \\frac{-(x_i - \\mu)^2}{2\\sigma^2} \\right \\} \\right ).$$\n",
    "**`for` loops are not allowed!**\n",
    "\n",
    "Hint: You should rewrite the log of the Gaussian as in the lectures i.e. expand the log of a product etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FILL THIS IN: Note that data is an ND vector, mu is a scalar, and so is sigma\n",
    "def nll(data, mu, sigma):\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we show a plot of the NLL as a function of varying $\\mu$ and $\\sigma$ given the generated data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Domain of the parameters space\n",
    "muspace = np.linspace(-10, 10, num=200)\n",
    "sigmaspace = np.linspace(0.1, 10, num=200)\n",
    "\n",
    "# Compute the value of the nll at each parameter value: note this is an inefficient method\n",
    "nlls = []\n",
    "for m in muspace:\n",
    "    for s in sigmaspace:\n",
    "        nlls.append(nll(data, m, s))\n",
    "nlls = np.asarray(nlls).reshape(200,200)\n",
    "\n",
    "NLL_min = np.amin(nlls)\n",
    "ind = np.unravel_index(np.argmin(nlls, axis=None), nlls.shape)\n",
    "mean_brute = muspace[ind[0]]\n",
    "std_brute = sigmaspace[ind[1]]\n",
    "\n",
    "plt.figure(1, figsize=(15,5))\n",
    "X, Y = np.meshgrid(muspace, sigmaspace)\n",
    "plt.contour(X,Y,np.exp(-nlls.T))\n",
    "\n",
    "# Plot the true parameters with a blue circle\n",
    "plt.scatter(mean_true, np.sqrt(var_true), color='b', marker='o')\n",
    "\n",
    "# Plot the brute force ML parameters with a green cross\n",
    "plt.scatter(mean_brute, std_brute, color='g', marker='x')\n",
    "\n",
    "plt.xlabel('$\\mu$', fontsize=16)\n",
    "plt.ylabel('$\\sigma$', fontsize=16)\n",
    "plt.title('The NLL', fontsize=16)\n",
    "plt.legend(['True parameters', 'Brute force ML parameters'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Method 1\n",
    "Now we are going to compute the maximum likelihood (ML) mean and variance through gradient descent on the averaged negative log-likelihood of the parameters given the data. At iteration $t$ you will update the parameters $\\mu_t$ or $\\sigma_t$ (jointly referred to as $\\theta_t$) according to the equation\n",
    "\n",
    "$$\\theta_{t+1} \\gets \\theta_t - \\lambda \\left . \\frac{\\partial \\mathcal{L}}{\\partial \\theta} \\right |_{\\theta=\\theta_t}$$\n",
    "\n",
    "At iteration $t$ we denote the loss $\\mathcal{L}_t = \\mathcal{L}(\\mu_t, \\sigma_t)$. The optimal loss is $\\mathcal{L}^* = \\mathcal{L}(\\mu^*, \\sigma^*)$.\n",
    "\n",
    "**`for` loops are not allowed!**\n",
    "\n",
    "**TODO** Accustom yourself with the code below, then fill in the missing parts. You need to fill in:\n",
    "- `dLdmu(x, mu, sigma)`: takes in an array `X` of $N$ data points $\\{x_i\\}$, a scalar mean `mu`, and a scalar `sigma`. It should return a scalar for the gradient of the averaged negative log-likelihood wrt the mean i.e. $$\\texttt{dLdmu(X,mu,sigma)} = -\\frac{\\partial}{\\partial \\mu} \\left ( \\mathcal{L}(\\mu,\\sigma) \\right)$$\n",
    "- `dLdsigma(x, mu, sigma)`: takes in an array `X` of $N$ data points $\\{x_i\\}$, a scalar mean `mu`, and a scalar `sigma`. It should return a scalar for the gradient of the averaged negative log-likelihood wrt the standard deviation i.e. $$\\texttt{dLdsigma(X,mu,sigma)} = -\\frac{\\partial}{\\partial \\sigma} \\left ( \\mathcal{L}(\\mu,\\sigma) \\right)$$\n",
    "- `update_parameter(learning_rate, current_parameter, update)`: takes in a `learning_rate`, the current parameter `mu` or `sigma` and an `update` and updates according to the gradient descent rule."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FILL THIS IN\n",
    "def dLdmu(X, mu, sigma):\n",
    "    return 0\n",
    "\n",
    "# FILL THIS IN\n",
    "def dLdsigma(X, mu, sigma):\n",
    "    return 0\n",
    "\n",
    "# FILL THIS IN\n",
    "def update_parameter(learning_rate, current_parameter, update):\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Randomly initialize the mean and variance\n",
    "mu0 = 20*(np.random.rand()-0.5)\n",
    "sigma0 = 10*np.random.rand()\n",
    "mu = mu0\n",
    "sigma = sigma0\n",
    "\n",
    "# These lists will store the trajectories of the parameters and loss\n",
    "mus = [mu,]\n",
    "sigmas = [sigma,]\n",
    "losses = []\n",
    "\n",
    "# Optimization meta-parameters\n",
    "n_iter = 1000\n",
    "learning_rate = 1e-2\n",
    "\n",
    "# The optimization loop\n",
    "for t in range(n_iter):\n",
    "    loss = nll(data, mu, sigma)\n",
    "    mu_new = update_parameter(learning_rate, mu, dLdmu(data, mu, sigma))\n",
    "    sigma_new = update_parameter(learning_rate, sigma, dLdsigma(data, mu, sigma))\n",
    "    \n",
    "    mu = mu_new\n",
    "    sigma = sigma_new\n",
    "    \n",
    "    # Store the trajectories\n",
    "    mus.append(mu)\n",
    "    sigmas.append(sigma)\n",
    "    losses.append(loss)\n",
    "\n",
    "# Convert lists to numpy arrays\n",
    "mus = np.asarray(mus)\n",
    "sigmas = np.asarray(sigmas)\n",
    "losses = np.asarray(losses)\n",
    "\n",
    "# Output of gradient descent\n",
    "final_nll = losses[-1]\n",
    "muend = mus[-1]\n",
    "sigmaend = sigmas[-1]\n",
    "print(\"Final NLL: {}\".format(final_nll))\n",
    "\n",
    "###############################################################\n",
    "# Plotting\n",
    "plt.figure(2, figsize=(15,5))\n",
    "# Left figure: trajectory of the ML mean and std over time\n",
    "plt.contour(X,Y,np.exp(-nlls.T))\n",
    "\n",
    "# Plot true parameters and ML parameters\n",
    "plt.scatter(mean_true, np.sqrt(var_true), color='b', marker='o')\n",
    "\n",
    "# Plot trajectory ends\n",
    "plt.scatter(mu0, sigma0, color='r', marker='+')\n",
    "plt.scatter(muend, sigmaend, color='r', marker='*')\n",
    "\n",
    "# Plot trajectory\n",
    "plt.plot(mus, sigmas)\n",
    "plt.scatter(mean_brute, std_brute, color='g', marker='x')\n",
    "\n",
    "# Labelling\n",
    "plt.xlabel('$\\mu$', fontsize=16)\n",
    "plt.ylabel('$\\sigma$', fontsize=16)\n",
    "plt.title('The NLL with trajectory', fontsize=16)\n",
    "plt.legend(['True parameters', 'Start of trajectory', 'End of trajectory', 'Brute force parameters'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Play about with the learning rate and the number of iterations. \n",
    "1) If the learning rate is too high or too small what happens? \n",
    "2) If the number of iterations is too high or too small, what happens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write your answer here:..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Method 2\n",
    "Now we are going to compute the maximum likelihood mean and variance through moment matching. We can use this method to verify that the solution our gradient descent optimizer has converged to is correct\n",
    "\n",
    "**TODO** Fill in the expressions for `muML` and `sigmaML`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FILL THIS IN\n",
    "muML = 0\n",
    "\n",
    "# FILL THIS IN\n",
    "sigmaML = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nllML = nll(data, muML, sigmaML)\n",
    "\n",
    "# Plotting\n",
    "plt.figure(3, figsize=(15,12))\n",
    "\n",
    "# Top left figure: trajectory\n",
    "plt.subplot(2,2,1)\n",
    "plt.contour(X,Y,np.exp(-nlls.T))\n",
    "plt.scatter(mean_true, np.sqrt(var_true), color='b', marker='o')\n",
    "plt.scatter(mean_brute, std_brute, color='g', marker='x')\n",
    "\n",
    "# Plot trajectory ends\n",
    "plt.scatter(mu0, sigma0, color='r', marker='+')\n",
    "plt.scatter(muend, sigmaend, color='r', marker='*')\n",
    "plt.scatter(muML, sigmaML, color='k', marker='x', s=500)\n",
    "plt.legend(['True params.', 'Brute params.', 'Start', 'End', 'ML params.'])\n",
    "\n",
    "plt.plot(mus, sigmas)\n",
    "\n",
    "# Labelling\n",
    "plt.xlabel('$\\mu$', fontsize=16)\n",
    "plt.ylabel('$\\sigma$', fontsize=16)\n",
    "plt.title('The NLL with trajectory and analytical solution', fontsize=16)\n",
    "\n",
    "plt.subplot(2,2,2)\n",
    "# Top right figure: difference in loss from optimum\n",
    "plt.semilogy(losses - nllML)\n",
    "plt.xlabel('Iteration $t$', fontsize=16)\n",
    "plt.ylabel('$\\log(\\mathcal{L}_t - \\mathcal{L}^*)$', fontsize=16)\n",
    "plt.title('Log error with iteration', fontsize=16)\n",
    "\n",
    "plt.subplot(2,2,3)\n",
    "# Bottom left figure: L2 distance between trajectory parameters and ML solution\n",
    "parameter_distance = np.sqrt((mus - muML)**2 + (sigmas - sigmaML)**2)\n",
    "plt.semilogy(parameter_distance)\n",
    "plt.xlabel('Iteration $t$', fontsize=16)\n",
    "plt.ylabel('$\\log \\| [\\mu_t,\\sigma_t] - [\\mu_*, \\sigma_*] \\|_2)$', fontsize=16)\n",
    "plt.title('Log parameter distance with iteration', fontsize=16)\n",
    "\n",
    "end_dist = np.sqrt((mean_brute - muML)**2 + (std_brute - sigmaML)**2)\n",
    "print(\"True parameters vs. brute force parameters distance: {}\".format(end_dist))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2\n",
    "You have written a gradient descent optimizer to numerically compute the optimum of the Gaussian likelihood. You also computed the ML estimates of the parameters and compared the two. Now you are going to write a gradient descent optimizer to optimize a 1D logistic regressor.\n",
    "\n",
    "For the next few boxes, read and run the code until the next **TODO**\n",
    "\n",
    "### Logistic regression\n",
    "In logistic regression, we are performing a classification task. Given an input $x_i \\in \\mathbb{R}$, there is an associated 'label' $y_i$ which can take on values in $\\{0,1\\}$. The two possible outcomes refer to two 'classes'. The model used is a Bernoulli distribution, \n",
    "\n",
    "$$P(y | x, w, b) = \\text{Bernoulli}(y | wx + b) = \\sigma(wx+b)^y (1 - \\sigma(wx+b))^{1-y},$$\n",
    "\n",
    "where $\\sigma(x) = \\frac{1}{1 + e^{-x}}$ is a sigmoid function. It looks like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.linspace(-5,5,num=100)\n",
    "y = 1 / (1 + np.exp(-x))\n",
    "\n",
    "plt.figure(4, figsize=(15,5))\n",
    "plt.plot(x,y)\n",
    "plt.xlim(-5,5)\n",
    "plt.ylim(0,1)\n",
    "plt.xlabel('$x$', fontsize=16)\n",
    "plt.ylabel('$y = \\sigma(x)$', fontsize=16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model has two parameters $w,b$, which are both real numbers. The *slope* $w$ controls the gradient of the sigmoid and the *bias* $b$ controls the position of the sigmoid. See examples below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.linspace(-100,100,num=500)\n",
    "y = 1 / (1 + np.exp(-x))\n",
    "\n",
    "plt.figure(5, figsize=(15,5))\n",
    "ws = [1,2,0.1,1]\n",
    "bs = [0,0,0,-3]\n",
    "for i in range(4):\n",
    "    plt.subplot(1,4,i+1)\n",
    "    plt.plot(ws[i]*x+bs[i],y)\n",
    "    plt.xlim(-5,5)\n",
    "    plt.ylim(0,1)\n",
    "    plt.xlabel('$x$', fontsize=16)\n",
    "    if i == 0:\n",
    "        plt.ylabel('$y = \\sigma(wx+b)$', fontsize=16)\n",
    "    plt.title('$w=${}, $b=${}'.format(ws[i], bs[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The averaged negative log-likelihood of this model is then\n",
    "\n",
    "\\begin{align}\\mathcal{L}(w,b) &= -\\frac{1}{N} \\sum_{i=1}^N \\log P(y_i | x_i, w, b) \\\\\n",
    "&= -\\frac{1}{N} \\sum_{i=1}^N \\log (\\sigma(wx_i+b))^{y_i} (1 - \\sigma(wx_i+b))^{1-y_i} \\\\\n",
    "&= -\\frac{1}{N} \\sum_{i=1}^N y_i \\log (\\sigma(wx_i+b)) + (1-y_i)\\log(1 - \\sigma(wx_i+b)) \\\\\n",
    "\\end{align}\n",
    "\n",
    "You are given data $\\mathcal{D}=\\{(x_i, y_i)\\}$, generated from a distribution as shown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 100\n",
    "wtrue = 5+2*np.random.rand()\n",
    "btrue = np.random.randn()\n",
    "\n",
    "xdata = 4*(np.random.rand(N)-0.5)\n",
    "xdata = np.sort(xdata)\n",
    "\n",
    "\n",
    "ytrue = 1 / (1 + np.exp(-(wtrue*xdata + btrue)))\n",
    "ydata = (np.random.rand(N) + ytrue) > 1\n",
    "\n",
    "plt.figure(6, figsize=(15,5))\n",
    "plt.plot(xdata,ytrue)\n",
    "plt.scatter(xdata[ydata==0], ydata[ydata==0], color='b')\n",
    "plt.scatter(xdata[ydata==1], ydata[ydata==1], color='r')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO** Write the averaged negative log-likelihood function in terms of $w$ and $b$. Note that computing $\\log (\\sigma (x))$ is actually unstable, and so we have included a stable `log_sigmoid` function for you. Note also that $\\log (1 - \\sigma (x)) = \\log(\\sigma(-x))$.\n",
    "\n",
    "**`for` loops are not allowed!**\n",
    "\n",
    "Hint: It helps to write the averaged NLL in two steps:\n",
    "\n",
    "\\begin{align}\n",
    "    z_i &= wx_i + b \\\\\n",
    "    \\mathcal{L}(w,b) &= -\\frac{1}{N} \\sum y_i \\log \\sigma (z_i) + (1 - y_i) \\log (1-\\sigma (z_i))\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_sigmoid(x):\n",
    "    return -np.log(1+np.exp(-x))\n",
    "\n",
    "# FILL THIS IN\n",
    "def nll(xdata, ydata, w, b):\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Domain of the parameters space\n",
    "wspace = np.linspace(0, 25, num=200)\n",
    "bspace = np.linspace(-5, 5, num=200)\n",
    "\n",
    "# Compute the value of the nll at each parameter value: note this is an inefficient method\n",
    "nlls = []\n",
    "for w in wspace:\n",
    "    for b in bspace:\n",
    "        nlls.append(nll(xdata, ydata, w, b))\n",
    "nlls = np.asarray(nlls).reshape(200,200)\n",
    "\n",
    "NLL_min = np.amin(nlls)\n",
    "ind = np.unravel_index(np.argmin(nlls, axis=None), nlls.shape)\n",
    "w_brute = wspace[ind[0]]\n",
    "b_brute = bspace[ind[1]]\n",
    "\n",
    "plt.figure(7, figsize=(15,5))\n",
    "X, Y = np.meshgrid(wspace, bspace)\n",
    "plt.contour(X,Y,np.exp(-nlls.T))\n",
    "\n",
    "# Plot the true parameters with a blue circle\n",
    "plt.scatter(wtrue, btrue, color='b', marker='o')\n",
    "\n",
    "# Plot the brute force ML parameters with a green cross\n",
    "plt.scatter(w_brute, b_brute, color='g', marker='x')\n",
    "\n",
    "plt.xlabel('$w$', fontsize=16)\n",
    "plt.ylabel('$b$', fontsize=16)\n",
    "plt.title('The NLL', fontsize=16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient descent on the NLL\n",
    "Now we are going to compute the maximum likelihood (ML) slope and bias through numerical optimization of the averaged negative log-likelihood of the parameters given the data. Don't worry, **we have written down the gradient expressions for you**. At iteration $t$ you will update the parameters $w_t$ or $b_t$ (jointly referred to as $\\theta_t$) according to the equation\n",
    "\n",
    "$$\\theta_{t+1} \\gets \\theta_t - \\lambda \\left . \\frac{\\partial \\mathcal{L}}{\\partial \\theta} \\right |_{\\theta=\\theta_t}$$\n",
    "\n",
    "At iteration $t$ we denote the loss $\\mathcal{L}_t = \\mathcal{L}(w_t, b_t)$. The optimal loss is $\\mathcal{L}^* = \\mathcal{L}(w^*, b^*)$.\n",
    "\n",
    "**TODO** Accustom yourself with the code below, then fill in the missing parts. You need to fill in:\n",
    "- `sigmoid`: takes in a scalar or array of numbers `x`, returns $\\sigma(x) = 1/(1+e^{-x})$\n",
    "- `dLdw(xdata, ydata, w, b)`: takes in an array `xdata` of $N$ data points $\\{x_i\\}$ with an array of labels `ydata` for $\\{y_i\\}$, a slope `w`, and a scalar `b`. It should return a scalar for the gradient of the averaged negative log-likelihood wrt the slope i.e. \n",
    "$$\\texttt{dLdw(xdata,ydata,w,b)} = -\\frac{\\partial}{\\partial w} \\left ( \\mathcal{L}(w,b) \\right) = -\\frac{1}{N} \\sum_{i=1}^N x_i(y_i - \\sigma(wx_i + b))$$\n",
    "- `dLdb(xdata, ydata, w, b)`: takes in an array `xdata` of $N$ data points $\\{x_i\\}$ with an array of labels `ydata` for $\\{y_i\\}$, a slope `w`, and a scalar `b`. It should return a scalar for the gradient of the averaged negative log-likelihood wrt the bias i.e. \n",
    "$$\\texttt{dLdb(xdata,ydata,w,b)} = -\\frac{\\partial}{\\partial b} \\left ( \\mathcal{L}(w,b) \\right) = -\\frac{1}{N} \\sum_{i=1}^N (y_i - \\sigma(wx_i + b))$$\n",
    "\n",
    "**It would be a good exercise for you to work out these gradients from the NLL for yourself, but you do not need to hand them in.**\n",
    "\n",
    "Note you may need to play around with the learning rate and number of iteration to get this to converge. If it gets stuck for a long time, just resample the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FILL THIS IN\n",
    "def sigmoid(x):\n",
    "    return 0\n",
    "\n",
    "# FILL THIS IN\n",
    "def dLdw(xdata, ydata, w, b):\n",
    "    return 0\n",
    "\n",
    "# FILL THIS IN\n",
    "def dLdb(xdata, ydata, w, b):\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Randomly initialize the mean and variance\n",
    "w0 = 25*np.random.rand()\n",
    "b0 = 10*(np.random.rand()-0.5)\n",
    "\n",
    "w = w0\n",
    "b = b0\n",
    "\n",
    "# These lists will store the trajectories of the parameters and loss\n",
    "ws = [w0,]\n",
    "bs = [b0,]\n",
    "losses = []\n",
    "\n",
    "# Optimization meta-parameters\n",
    "learning_rate = 1e-1\n",
    "n_iter = 1000\n",
    "\n",
    "# The optimization loop\n",
    "for t in range(n_iter):\n",
    "    loss = nll(xdata, ydata, w, b)\n",
    "    w_new = update_parameter(learning_rate, w, dLdw(xdata, ydata, w, b))\n",
    "    b_new = update_parameter(learning_rate, b, dLdb(xdata, ydata, w, b))\n",
    "    \n",
    "    w = w_new\n",
    "    b = b_new\n",
    "    \n",
    "    ws.append(w)\n",
    "    bs.append(b)\n",
    "    losses.append(loss)\n",
    "\n",
    "    \n",
    "ws = np.asarray(ws)\n",
    "bs = np.asarray(bs)\n",
    "losses = np.asarray(losses)\n",
    "\n",
    "final_nll = losses[-1]\n",
    "wend = ws[-1]\n",
    "bend = bs[-1]\n",
    "print(\"Final NLL: {}\".format(final_nll))\n",
    "\n",
    "###############################################################\n",
    "# Plotting\n",
    "plt.figure(8, figsize=(15,5))\n",
    "# Left figure: trajectory of the ML mean and std over time\n",
    "plt.contour(X,Y,np.exp(-nlls.T))\n",
    "\n",
    "# Plot the true parameters with a blue circle\n",
    "plt.scatter(wtrue, btrue, color='b', marker='o')\n",
    "\n",
    "# Plot the brute force ML parameters with a green cross\n",
    "plt.scatter(w_brute, b_brute, color='g', marker='x')\n",
    "\n",
    "# Plot trajectory\n",
    "plt.plot(ws, bs)\n",
    "plt.scatter(ws[0], bs[0], color='r', marker='+')\n",
    "\n",
    "# Labelling\n",
    "plt.xlabel('$w$', fontsize=16)\n",
    "plt.ylabel('$b$', fontsize=16)\n",
    "plt.title('The NLL with trajectory', fontsize=16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This last bit of code compares the result of your gradient descent algorithm with the output of a more advanced optimizer from the `scipy.optimize` library. Have a look at the library if you are interested. You should notice that gradient descent has highly variable rates of convergence to the optimum. Can you think of intuitive reasons why? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fun = lambda x: nll(xdata, ydata, x[0], x[1])\n",
    "res = minimize(fun, [w0,b0])\n",
    "wML = res['x'][0]\n",
    "bML = res['x'][1]\n",
    "\n",
    "nllML = nll(xdata, ydata, wML, bML)\n",
    "\n",
    "# Plotting\n",
    "plt.figure(9, figsize=(15,12))\n",
    "\n",
    "# Top left figure: trajectory\n",
    "plt.subplot(2,2,1)\n",
    "plt.contour(X,Y,np.exp(-nlls.T))\n",
    "plt.scatter(wtrue, btrue, color='b', marker='o')\n",
    "plt.scatter(w_brute, b_brute, color='g', marker='x')\n",
    "\n",
    "# Plot trajectory ends\n",
    "plt.scatter(w0, b0, color='r', marker='+')\n",
    "plt.scatter(wend, bend, color='r', marker='*')\n",
    "plt.scatter(wML, bML, color='k', marker='x', s=500)\n",
    "plt.legend(['True params.', 'Brute params.', 'Start', 'End', 'ML params.'])\n",
    "\n",
    "plt.plot(ws, bs)\n",
    "\n",
    "# Labelling\n",
    "plt.xlabel('$w$', fontsize=16)\n",
    "plt.ylabel('$b$', fontsize=16)\n",
    "plt.title('The NLL with trajectory and analytical solution', fontsize=16)\n",
    "\n",
    "plt.subplot(2,2,2)\n",
    "# Top right figure: difference in loss from optimum\n",
    "plt.semilogy(losses - nllML)\n",
    "plt.xlabel('Iteration $t$', fontsize=16)\n",
    "plt.ylabel('$\\log(\\mathcal{L}_t - \\mathcal{L}^*)$', fontsize=16)\n",
    "plt.title('Log error with iteration', fontsize=16)\n",
    "\n",
    "plt.subplot(2,2,3)\n",
    "# Bottom left figure: L2 distance between trajectory parameters and ML solution\n",
    "parameter_distance = np.sqrt((ws - wML)**2 + (bs - bML)**2)\n",
    "plt.semilogy(parameter_distance)\n",
    "plt.xlabel('Iteration $t$', fontsize=16)\n",
    "plt.ylabel('$\\log \\| [w_t,b_t] - [w_*, b_*] \\|_2)$', fontsize=16)\n",
    "plt.title('Log parameter distance with iteration', fontsize=16)\n",
    "\n",
    "end_dist = np.sqrt((w_brute - wML)**2 + (b_brute - bML)**2)\n",
    "print(\"True parameters vs. brute force parameters distance: {}\".format(end_dist))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
