{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 5\n",
    "\n",
    "\n",
    "* It is expected that you work individually and the usual plagarism rules apply.\n",
    "* Submissions are to be made on canvas. Make sure that you add your student ID in the submission comments.\n",
    "* The main notebook file you submit should read \"Lab[number]_[last name].ipynb\", for example \"Lab2_Bongers.ipynb\". \n",
    "* Please make sure your code will run without problems\n",
    "\n",
    "_You need to fill in everywhere that there is a_ '__TODO__'\n",
    "\n",
    "Feel free ask any questions during the computer lab sessions, or email the TA.\n",
    "\n",
    "**The due date for the labs is next Wednesday at 14:59**\n",
    "\n",
    "## Part 1\n",
    "\n",
    "\n",
    "In this part of the lab you will plot the prior, posterior and predictive distributions for a set of data that we assume to follow an exponential distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.stats as st\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import statsmodels.datasets\n",
    "\n",
    "from scipy import integrate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Statsmodels is a Python package for conducting statistical data analyses. It also contains real-world datasets that we can use when experimenting with new methods. Here, we load the heart dataset. Each line indicates the number of days that a patient survived after a heart transplant, together with the age of the patient.\n",
    "\n",
    "Here you can have a look at the 5 last lines of the data frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>survival</th>\n",
       "      <th>censors</th>\n",
       "      <th>age</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>14.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>40.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>167.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>26.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>110.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>23.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>13.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>28.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>35.2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    survival  censors   age\n",
       "64      14.0      1.0  40.3\n",
       "65     167.0      0.0  26.7\n",
       "66     110.0      0.0  23.7\n",
       "67      13.0      0.0  28.9\n",
       "68       1.0      0.0  35.2"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_original = statsmodels.datasets.heart.load_pandas().data\n",
    "data_original.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This dataset contains censored and uncensored data: a censor of $0$ means that the patient was alive at the end of the study, and thus we don't know the exact survival time. We only know that the patient survived at least the indicated number of days. For simplicity here, we only keep uncensored data (we thereby introduce a bias toward patients that did not survive very long after their transplant) and we store it in an array 'data_full'. We also divided each number by 365, so the period of time that a patient survived a heart transplant is given in years."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_original = data_original[data_original.censors == 1]\n",
    "survival = data_original.survival\n",
    "survival=np.array(survival)\n",
    "data_full=survival/365"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the array 'data', we store the first 'p' elements of 'data_complete'. First, we let 'p' be $10$. The idea is we are going to predict the distribution of the survival time only using 'p' data (while the full dataset contains $69$ data) and later, we can play with the value of 'p' and see how many data are actually needed to have a satisfying prediction. We also plot the data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5,1,'Scatter plot of training data $\\\\mathcal{D}$')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# restricting the data to p elements\n",
    "p=10\n",
    "data=data_full[:p]\n",
    "N=len(data)\n",
    "\n",
    "# plotting the data\n",
    "datamin = np.amin(data)\n",
    "datamax = np.amax(data)\n",
    "absmax = np.maximum(np.abs(datamin), np.abs(datamax)) + 1\n",
    "plt.figure(0)\n",
    "plt.scatter(data, np.zeros(data.shape))\n",
    "plt.xlim(-absmax, absmax)\n",
    "plt.title('Scatter plot of training data $\\mathcal{D}$', fontsize=12)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We assume that the data follows an exponential distribution with parameter $\\lambda$. That is,\n",
    "$$\n",
    "p(x|\\lambda)=\\lambda e^{-\\lambda x}\n",
    "$$\n",
    "We also suppose that the prior distribution for $\\lambda$ is a Gamma distribution with parameters $\\alpha$ and $\\beta$\n",
    "$$\n",
    "p(\\lambda | \\alpha, \\beta)= \\frac{1}{Z(\\alpha,\\beta)} x^{\\alpha-1} e^{-\\beta x}\n",
    "$$\n",
    "Initially, we suppose $\\alpha=2$ and $\\beta=2$. We start by plotting the prior distribution. The code below plots the function\n",
    "$$\n",
    "x^{\\alpha-1} e^{-\\beta x}\n",
    "$$\n",
    "So if we would normalize that function, we would obtain the prior distribution. It is *almost* the prior, the problem being that the area under the graph might not be $1$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#plotting the prior up to a constant (not normalized)\n",
    "\n",
    "def gam(x,alpha=1.1,beta=3):\n",
    "    return x**(alpha-1)*np.exp(-x*beta)\n",
    "\n",
    "alpha=1.1\n",
    "beta=3\n",
    "x=np.linspace(0,8,50)\n",
    "plt.plot(x,gam(x, alpha=alpha, beta=beta))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One way from the previous graph to obtain the prior distribution is to define a function `area(func,a,b)` that computes the area under the graph of the function `func` in the interval $[a,b]$. Then we define a function `norm_plot(func,a,b,legend)` which given a function `func` and an interval $[a,b]$, plots the *normalized* graph of `func` in the interval $[a,b]$ and write `legend` as a legend of the graph. You don't have to write the code for `norm_plot(func,a,b,legend)`, but make sure you understand how it works.  \n",
    "\n",
    "__TODO1__ (0.5 pt) Define the function `area(func,a,b)`. In order to compute the area under the graph of a function, we use integration. You can use the following built-in function to define  `area(func,a,b)`:\n",
    "https://docs.scipy.org/doc/scipy/reference/generated/scipy.integrate.quad.html#scipy.integrate.quad\n",
    "\n",
    "Make sure that the output is indeed a scalar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "expected an indented block (<ipython-input-6-8fdf35e506f0>, line 5)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-6-8fdf35e506f0>\"\u001b[1;36m, line \u001b[1;32m5\u001b[0m\n\u001b[1;33m    def norm_plot(func,a,b,legend):\u001b[0m\n\u001b[1;37m      ^\u001b[0m\n\u001b[1;31mIndentationError\u001b[0m\u001b[1;31m:\u001b[0m expected an indented block\n"
     ]
    }
   ],
   "source": [
    "def area(func,a,b):\n",
    "    #to be filled in by the students\n",
    "    \n",
    "\n",
    "def norm_plot(func,a,b,legend):\n",
    "    x=np.linspace(a,b,100)\n",
    "    norm_func=func(x)*(1/area(func,a,b))\n",
    "    plt.plot(x,norm_func,label=legend)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__TODO2__(0.5 pt) Using the function `norm_plot`, plot the prior distribution in the interval $[0,8]$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot the prior\n",
    "\n",
    "\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The formula for calculating the posterior distribution is \n",
    "$$\n",
    "p(\\lambda|\\mathcal{D}) = \\frac{p(\\mathcal{D}|\\lambda)p(\\lambda)}{p(\\mathcal{D})}\n",
    "$$\n",
    "where $\\mathcal{D}$ is the training data. Before calculating analytically the posterior distribution for $\\lambda$, we will compute by brute force the value of $\\lambda$, which we call $\\lambda_{\\text{brute force}}$, such that $p(\\lambda_{\\text{brute force}}|\\mathcal{D})$ is maximized. \n",
    "\n",
    "Given the above equation, we know that\n",
    "$$\n",
    "p(\\lambda|\\mathcal{D}) \\propto p(\\mathcal{D}|\\lambda)p(\\lambda)\n",
    "$$\n",
    "So we have to find $\\lambda_{\\text{brute force}}$ such that \n",
    "$$\n",
    "p(\\mathcal{D}|\\lambda_{\\text{brute force}})p(\\lambda_{\\text{brute force}})\n",
    "$$\n",
    "is maximized. \n",
    "\n",
    "The idea is that for *enough* $\\lambda_1, \\dots,\\lambda_k$, we compute the array `A`\n",
    "$$\n",
    "\\texttt{A} = \\begin{bmatrix}\n",
    "p(\\mathcal{D}|\\lambda_1)p(\\lambda_1) \\\\\n",
    "p(\\mathcal{D}|\\lambda_2)p(\\lambda_2) \\\\\n",
    "\\dots \\\\\n",
    "p(\\mathcal{D}|\\lambda_k)p(\\lambda_k)\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "and then using the `argmax` function (as in a previous assignment), we find the $\\lambda_i$ corresponding to the highest value in `A`. This $\\lambda_i$ is our $\\lambda_{\\text{brute force}}$. \n",
    "\n",
    "__TODO3__ (1 pt) Write a program that outputs the value of the variable `lambda_brute_force`, which should be $\\lambda_{\\text{brute force}}$. It should follow the steps of the method described above. In order to generate the set $\\{ \\lambda_1, \\dots,\\lambda_k \\}$, you can use `np.linspace(0,10,k)`. Then define the array `A` as above. Note that $p(\\mathcal{D}|\\lambda_j)$ refers to an exponential distribution with parameter $\\lambda_j$ and $p(\\lambda_j)$ refers to a gamma distribution with parameters $\\alpha$ and $\\beta$ ($\\alpha$ and $\\beta$ have the same values as before). Finally, using `argmax`, find the $\\lambda_i$ corresponding to the highest value of `A`. \n",
    "\n",
    "__BONUS__ (1.5 pt) Write the program without using any `for` loops."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k=50\n",
    "\n",
    "\n",
    "\n",
    "#given\n",
    "print(\"The brute force estimator of lambda is: \", lambda_brute_force)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__TODO4__ (1 pt) Define `alpha_post` and `beta_post` as the parameters of the posterior distribution (the formulas for `alpha_post` and `beta_post` are the answers from the first question of the tutorial). Plot the posterior distribution using the function `norm_plot`. Note that this time, you cannot use the function `gam` as this function has parameters $\\alpha$ and $\\beta$ (and the parameters should be `alpha_post` and `beta_post`). So you need to define a new function as an input for `norm_plot`. \n",
    "\n",
    "We also plotted for you the prior distribution and the brute force estimator. So you can compare those with the posterior distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#plot the posterior distribution, to be filled in\n",
    "\n",
    "\n",
    "#plot the prior distribution, given\n",
    "norm_plot(gam,0,8,'Prior')\n",
    "\n",
    "#plot the brute force operator, given\n",
    "plt.plot(20*[lambda_brute_force],np.linspace(0,1.2,20),label='Brute force estimator')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__TODO5__ (0.5 pt) Try again to run the above program with different values for `p` and for $\\alpha$ and $\\beta$. Starting from which value of `p`, are we starting to have a stable prediction? (that is, how much data is sufficient to have already a satisfying prediction?) What happens when we start with different values of $\\alpha$ and $\\beta$? Does it affect a lot the posterior distribution?\n",
    "\n",
    "__TODO6__ (1 pt) Draw a random sample $S$ of size `M`from the posterior distribution $p(\\lambda | \\mathcal{D})$. Then, for each value of $\\lambda$ in $S$, draw the exponential distribution with parameter $\\lambda$. In order to draw the random sample, you can use the following built-in function\n",
    "https://docs.scipy.org/doc/numpy-1.15.0/reference/generated/numpy.random.gamma.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#represent various p(x|lambda) for different lambda ~ p(lambda|data)\n",
    "M=5\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We compute now the maximum a posteriori (MAP) estimator of $\\lambda$, which is the mode of the posterior distribution. In case of a gamma distribution with parameters $\\alpha$ and $\\beta$, the mode is given by\n",
    "$$\n",
    "\\frac{\\alpha-1}{\\beta}\n",
    "$$\n",
    "\n",
    "__TODO7__ (1 pt) Compute the MAP estimator  $\\lambda_{MAP}$ of $\\lambda$. Compute also the maximum likelihood estimator $\\lambda_{ML}$ of $\\lambda$ (recall that it is equal to $\\frac{N}{\\sum_i x_i}$, where $\\{ x_1, \\dots, x_n\\}$ is the set of `data`). Then plot the exponential distribution with parameter $\\lambda_{MAP}$ and the exponential distribution with parameter $\\lambda_{ML}$. How do the two estimators compare to each other? And how do they compare with the brute force estimator?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#compute the MAP estimator of lambda using the theoretical formula, to be filled in\n",
    "\n",
    "\n",
    "#compute the ML estimator of lambda, to be filled in\n",
    "\n",
    "\n",
    "#print again the brute force estimator, given\n",
    "print('The brute force estimator of lambda is: ',lambda_brute_force)\n",
    "plt.plot(x,expo(x,lambda_brute_force),label='brute force $\\lambda$')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally we plot the predictive distribution. It is the distribution for future predicted test data based on the training data we have been given. So the posterior predictive distribution can be used to predict new data values. In the case of an exponential distribution for the data and a prior which is a gamma distribution, the predictive distribution is given by\n",
    "$$\n",
    "\\frac{\\alpha'(\\beta')^{\\alpha'}}{(\\beta'+x)^{\\alpha'+1}}\n",
    "$$\n",
    "where $\\alpha'$ and $\\beta'$ are the parameters of the posterior distribution. \n",
    "\n",
    "__TODO8__ (1 pt) Plot the predictive distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#compute the distributive and compare with p(x|lambda_MAP), given\n",
    "plt.plot(x,expo(x,lambda_MAP),label='exponential distr. with MAP $\\lambda$')\n",
    "\n",
    "#compute the predictive distribution, to be filled in\n",
    "\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2\n",
    "\n",
    "We will now do the same analysis but for a different distribution and slightly different type of problem.\n",
    "\n",
    "## Bayesian linear regression\n",
    "\n",
    "We consider the standard one dimensional linear regression problem\n",
    "$$\n",
    "y=wx+b+\\epsilon\n",
    "$$\n",
    "\n",
    "We assume that the noise $\\epsilon \\sim \\mathcal{N}(0,\\sigma^2)$. We are being given some data \n",
    "$$\n",
    "\\mathcal{D} =\\{ (x_1,y_1), \\dots, (x_N,y_N) \\}\n",
    "$$\n",
    "and we want to find estimators for the parameters $w$ and $b$ (note that here we assume $\\sigma^2$ to be known, there is also a bayesian analysis where $\\sigma^2$ is considered as an extra parameter to be estimated). \n",
    "\n",
    "First we randomly generate some data. The array `xdata` contains all the values $x_1, \\dots, x_N$ and the array `ydata`contains the values $y_1,\\dots,y_N$. We will also denote by $\\vec{y}$ the vector\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "y_1 \\\\\n",
    "\\dots \\\\\n",
    "y_N\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "and by $X$ the matrix:\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "x_1 & \\dots & x_N \\\\\n",
    "1 & \\dots & 1 \n",
    "\\end{bmatrix}\n",
    "$$\n",
    "(note that if you are looking at Section 3.3 of Bishop, this is the transpose of the design matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#randomly generate the data\n",
    "\n",
    "import random\n",
    "\n",
    "N_reg = 100\n",
    "wtrue = 5+2*np.random.rand()\n",
    "btrue = 1+np.random.randn()\n",
    "\n",
    "xdata = 4*(np.random.rand(N_reg)-0.5)\n",
    "xdata = np.sort(xdata)\n",
    "\n",
    "sigmatrue=2+np.random.randn()\n",
    "noisedata=np.random.normal(0, sigmatrue, N_reg)\n",
    "\n",
    "ydata= wtrue* xdata + btrue + noisedata\n",
    "plt.plot(xdata,ydata,\"r.\")\n",
    "\n",
    "print('The true parameters for w and b are', wtrue,'and', btrue)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have that\n",
    "$$\n",
    "y \\sim \\mathcal{N}(b,\\sigma^2)\n",
    "$$\n",
    "\n",
    "Usually we assume that the distribution of the parameters $(w,b)$ is a 2D gaussian, which will ensure that the posterior is also a gaussian. So\n",
    "$$\n",
    "p(w,b) = \\mathcal{N}(\\vec{\\mu},\\Sigma) \n",
    "$$\n",
    "\n",
    "\n",
    "First we plot the prior distribution (using contour lines). Make sure you understand the code. The prior distribution of the parameters $w$ and $b$ is a 2-dimensional gaussian with mean `m` and covariance matrix `cov`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import multivariate_normal\n",
    "\n",
    "#everything given\n",
    "m=[10,8]\n",
    "cov = [[4, 0], [0, 4]]\n",
    "\n",
    "N_w=200\n",
    "N_b=100\n",
    "w0 = np.linspace(5, 15, N_w)\n",
    "b0 = np.linspace(5,14,N_b)\n",
    "W, B = np.meshgrid(w0, b0)\n",
    "pos=np.dstack((W,B))\n",
    "\n",
    "plt.contour(W,B, multivariate_normal.pdf(pos,m,cov))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Like in the first part of the assignment, we will first compute brute force estimators of $w$ and $b$. The technique is the same but it is technically slightly more involved as we have 2 parameters instead of 1. We start with an array with *enough* different values of $w$ and $b$\n",
    "\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "[w_{11},b_{11}] & [w_{12},b_{12}] & \\dots & [w_{1k},b_{1k}] \\\\\n",
    "[w_{21},b_{21}] & [w_{22},b_{22}] & \\dots & [w_{2k},b_{2k}] \\\\\n",
    "\\dots & & & \\\\\n",
    "[w_{1l},b_{1l}] & [w_{1},b_{11}] & \\dots & [w_{11},b_{11}] \\\\\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "For this array, we will simply use the array `pos` from the previous program (it is a 3D array with dimension (N_b, N_w, 2) ).\n",
    "\n",
    "As in the case of the exponential distribution, for each $[w_{ij},b_{ij}]$, we will compute the value\n",
    "\n",
    "$$\n",
    "p(\\vec{y} \\;| \\; w_{ij},b_{ij},X)  p(w_{ij},b_{ij})\n",
    "$$\n",
    "and the brute force estimator will be the pair $(w_{ij},b_{ij})$ that maximizes that value.\n",
    "\n",
    "### Array_param\n",
    "\n",
    "In the array `array_param`, we will store all the values\n",
    "$$\n",
    " p(w_{ij},b_{ij})\n",
    "$$\n",
    "\n",
    "Note that `array_param` is thus a 2D array of shape `(N_b,N_w)`. \n",
    "\n",
    "Remember that the probability distribution of $(w,b)$ is a 2-dimensional gaussian with mean `m` (vector of size $2$) and $2 \\times 2$ covariance matrix `cov`. You can use any built-in function that you want. \n",
    "\n",
    "### Array_like\n",
    "\n",
    "Next in the array `array_like` (`like` as for likelihood), we will store all the values\n",
    "\n",
    "$$\n",
    "p(\\vec{y} \\;| \\; w_{ij},b_{ij},X) \n",
    "$$\n",
    "for each of the values of $[w_{ij},b_{ij}]$. Again  `array_like` is thus a 2D array of shape `(N_b,N_w)`.\n",
    "\n",
    "In case of linear regession, the likelihood is given by\n",
    "$$\n",
    "p(\\vec{y} \\;| \\; w_{ij},b_{ij},X) = \\mathcal{N}(X^T \\vec{w}_{ij}, \\sigma^2 I) \n",
    "$$\n",
    "where $I$ is the identity matrix of dimension $N$ and\n",
    "$$\n",
    "\\vec{w}_{ij} =\\begin{bmatrix} w_{ij} \\\\ b_{ij} \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "### Array B\n",
    "\n",
    "Finally, in the array `B`, we will store all the values\n",
    "$$\n",
    "p(\\vec{y} \\;| \\; w_{ij},b_{ij},X)  p(w_{ij},b_{ij})\n",
    "$$\n",
    "\n",
    "So, `B=np.multiply(array_param,array_like)`. Note that the 3 arrays `array_param`, `array_like` and `B`are arrays of shape `(N_b,N_w)`.\n",
    "\n",
    "Finally we have to find the value of $[w_{ij},b_{ij}]$ that corresponds to the maximum value of `B`. This can be done applying the `argmax` function to `B`. It is not as easy as in Part 1 though as for a given array `B` the result of `argmax`will be an index of the flattened array obtained from `B` (have a look at the documentation). In order to obtain the index of the maximum of the original unflattened `B`, you can for example use the function `unravel_index` from numpy (have also a look at the documentation to know how to use `unravel_index`).\n",
    "\n",
    "\n",
    "__TODO9__ (1.5 pt) Compute the brute force estimators of $w$ and $b$ using the method described above (and using the same names for the arrays)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#initialize the matrix X (needed for the likelihood later), given\n",
    "X=np.zeros((2,N_reg))\n",
    "X[0]=xdata\n",
    "X[1] =1.0\n",
    "\n",
    "#to be filled in "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now in order to find estimators of $w$ and $b$ (without using brute force), we will compute the posterior distribution of $(w,b)$ and take the maximum as an estimators of $(w,b)$.\n",
    "\n",
    "If you look in Bishop (section 3.3), you can see that the posterior distribution is given by\n",
    "$$\n",
    "p(w,b \\; | \\; \\sigma,\\mathcal{D}) \\sim \\mathcal{N}(\\vec{\\mu}_{post},\\Sigma_{post})\n",
    "$$\n",
    "where\n",
    "$$\n",
    "\\Sigma_{post}^{-1} = \\Sigma^{-1} + \\frac{1}{\\sigma^2} X X^T\n",
    "$$\n",
    "and \n",
    "$$\n",
    "\\vec{\\mu}_{post} = \\Sigma_{post} (\\Sigma^{-1} \\vec{\\mu} + \\frac{1}{\\sigma^2} X \\vec{y})\n",
    "$$\n",
    "\n",
    "\n",
    "__TODO10__ (1 pt) Using the same method as for plotting the prior, plot the posterior distribution of $(w,b)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__TODO11__ (0.5 pt) Compute the MAP estimator $(w_{MAP},b_{MAP})$ of $(w,b)$ (so that is the mode of the posterior distribution and in the case of a gaussian, this is the mean of the gaussian). Next plot the line with equation\n",
    "$$\n",
    "y= w_{MAP} x + b_{MAP}\n",
    "$$\n",
    "\n",
    "Did you see a time difference bewteen the computation of the estimators using brute force and using the Bayesian approach? What happens when you increase the values of `N_b` and `N_w`?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#given\n",
    "plt.plot(xdata,ydata,\"r.\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
